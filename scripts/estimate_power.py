#!/usr/bin/env python3
"""
estimate_power.py

Offline power/energy estimator for the VAD testbench.

What it does
------------
1) Standalone estimation:
   - Loads per-frame op-count profiles from outputs/op_profiles/<model>.json (generated by scripts/run_vad.py),
     or uses a built-in static profile for WebRTC VAD (L2/L3).
   - Multiplies op counts by per-op energy costs (nJ/op) to estimate:
       * energy per frame (nJ)
       * average power (mW)
       * average current (mA) at VDD

2) Offline cascade estimation:
   - Estimates a two-stage cascade average power:
         P_cascade â‰ˆ P_stage1 + rho_act * P_stage2
     where rho_act is computed directly from stage-1 clip predictions:
         rho_act = fraction of clips with pred == 1
     using clip result CSVs in outputs/clips/<TAG>/clip_results_<model>_<TAG>.csv

Key assumptions
---------------
- Hop size is hard-coded per user requirement:
    * WebRTC VAD: 10 ms hop
    * All other models: 16 ms hop
- Per-op energy costs are a proxy; for absolute numbers you should calibrate costs for your target silicon.
- Current is computed from power and VDD (defaults to 1.0 V).

Examples
--------
Standalone (models only):
  python scripts/estimate_power.py --models energy zcr energy_zcr webrtc2 --profile_dir outputs/op_profiles

Standalone + cascade over all tags found in outputs/clips:
  python scripts/estimate_power.py --models energy webrtc2 \
      --cascade energy:webrtc2 \
      --auto_tags \
      --profile_dir outputs/op_profiles

Cascade over specific tags:
  python scripts/estimate_power.py --cascade energy:webrtc2 \
      --tags musan_balanced librispeech_low_fp \
      --clips_root outputs/clips
"""

from __future__ import annotations

import argparse
import csv
import json
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import pandas as pd

# Matplotlib is optional; imported lazily only when plotting.
# (Avoids import errors on minimal environments.)
# import matplotlib.pyplot as plt


# ---------------------------------------------------------------------
# Default per-op energy costs (nJ/op). Proxy values; override via --costs_json.
# ---------------------------------------------------------------------
DEFAULT_COSTS: Dict[str, float] = {
    "add": 0.01,
    "mul": 0.05,
    "mac": 0.10,
    "cmp": 0.01,
    "sqrt": 0.20,
    "sigmoid": 0.30,
    "exp": 0.30,
    "log": 0.30,
    "div": 0.40,
    "abs": 0.01,
    "logic": 0.005,
}


# ---------------------------------------------------------------------
# Built-in static profiles for WebRTC levels (per frame).
# NOTE: These are coarse proxies. They are not derived from the same op-profiling
#       mechanism as other models.
# ---------------------------------------------------------------------
def get_webrtc_op_profile(level: int) -> Dict[str, int]:
    if level == 2:
        return {"mul": 4500, "add": 3000, "cmp": 800, "logic": 200}
    if level == 3:
        return {"mul": 5000, "add": 3300, "cmp": 1000, "logic": 200}
    raise ValueError("Unsupported WebRTC level (expected 2 or 3)")


# ---------------------------------------------------------------------
# Model name normalization and hop sizing
# ---------------------------------------------------------------------
def normalize_model_name(name: str) -> str:
    """
    Normalize model identifiers so they match:
      - op profile filenames: outputs/op_profiles/<model>.json
      - clip result filenames: clip_results_<model>_<tag>.csv

    Accepted aliases:
      webrtc2, webrtc_l2, webrtc_level2 -> webrtc_l2
      webrtc3, webrtc_l3, webrtc_level3 -> webrtc_l3
    """
    n = (name or "").strip().lower()
    if n in {"webrtc2", "webrtc_l2", "webrtc_level2", "webrtc-l2", "webrtc-lvl2"}:
        return "webrtc_l2"
    if n in {"webrtc3", "webrtc_l3", "webrtc_level3", "webrtc-l3", "webrtc-lvl3"}:
        return "webrtc_l3"
    return n


def hop_ms_for_model(model: str) -> float:
    m = normalize_model_name(model)
    return 10.0 if m.startswith("webrtc") else 16.0


# ---------------------------------------------------------------------
# Data structures
# ---------------------------------------------------------------------
@dataclass(frozen=True)
class OpProfile:
    model: str
    types: Dict[str, int]


@dataclass(frozen=True)
class PowerEstimate:
    model: str
    hop_ms: float
    frame_rate_hz: float
    total_ops_per_frame: int
    energy_per_frame_nj: float
    power_mw: float
    current_ma: float
    energy_per_hour_mj: float


# ---------------------------------------------------------------------
# Loading profiles
# ---------------------------------------------------------------------
def load_profile_from_json(profile_path: Path, model: str) -> OpProfile:
    """
    Expected JSON format (as produced by scripts/run_vad.py):
      {
        "<model>": {
          "frame_size": <int>,
          "hop_ms": <float>,
          "types": { "<op>": <count>, ... }
        }
      }
    """
    with open(profile_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    key = normalize_model_name(model)
    if isinstance(data, dict) and key in data:
        entry = data[key]
    elif isinstance(data, dict) and len(data) == 1:
        # Be robust to slight mismatches; take the only entry.
        entry = next(iter(data.values()))
    else:
        raise ValueError(f"Unexpected profile JSON structure in {profile_path}")

    if not isinstance(entry, dict) or "types" not in entry:
        raise ValueError(f"Profile {profile_path} missing required 'types' dict")

    types = entry["types"]
    if not isinstance(types, dict):
        raise ValueError(f"Profile {profile_path} has non-dict 'types' field")
    # Ensure int counts
    types_int = {str(op): int(cnt) for op, cnt in types.items()}
    return OpProfile(model=key, types=types_int)


def load_op_profile(model: str, profile_dir: Path) -> OpProfile:
    m = normalize_model_name(model)
    if m == "webrtc_l2":
        return OpProfile(model=m, types=get_webrtc_op_profile(2))
    if m == "webrtc_l3":
        return OpProfile(model=m, types=get_webrtc_op_profile(3))

    profile_path = profile_dir / f"{m}.json"
    if not profile_path.exists():
        # If profiles are stored under tag subdirectories, fall back to a recursive search
        candidates = list(profile_dir.rglob(f"{m}.json"))
        if candidates:
            candidates.sort(key=lambda p: p.stat().st_mtime, reverse=True)
            profile_path = candidates[0]
        else:
            raise FileNotFoundError(f"Op profile not found: {profile_path} (and no recursive matches under {profile_dir})")
    return load_profile_from_json(profile_path, m)


# ---------------------------------------------------------------------
# Estimation core
# ---------------------------------------------------------------------
def estimate_power(
    op_profile: OpProfile,
    costs_nj_per_op: Dict[str, float],
    vdd: float,
    hop_ms: Optional[float] = None,
    verbose: bool = False,
) -> Tuple[PowerEstimate, Dict[str, Dict[str, float]]]:
    """
    Returns:
      (PowerEstimate, per-op breakdown)
    """
    model = normalize_model_name(op_profile.model)
    hop = float(hop_ms if hop_ms is not None else hop_ms_for_model(model))
    frame_rate = 1000.0 / hop  # frames/sec

    energy_per_frame_nj = 0.0
    total_ops = 0
    breakdown: Dict[str, Dict[str, float]] = {}
    unknown_ops: List[str] = []

    for op, count in op_profile.types.items():
        total_ops += int(count)
        cost = float(costs_nj_per_op.get(op, 0.0))
        if op not in costs_nj_per_op:
            unknown_ops.append(op)
        e = float(count) * cost  # nJ
        breakdown[op] = {"count": int(count), "energy_nj": e, "cost_nj_per_op": cost}
        energy_per_frame_nj += e

        if verbose:
            print(f"[{model}] {op}: {count} * {cost} = {e:.6f} nJ")

    # Units:
    #   nJ/frame * frame/s = nJ/s = nW
    power_nw = energy_per_frame_nj * frame_rate
    power_mw = power_nw / 1e6

    if vdd <= 0:
        raise ValueError("--vdd must be > 0")
    current_ma = power_mw / vdd  # (mW / V) == mA

    # mW * 3600 s = mJ
    energy_per_hour_mj = power_mw * 3600.0

    if unknown_ops and verbose:
        unknown_ops = sorted(set(unknown_ops))
        print(f"[warn] {model}: unknown ops (treated as 0 cost): {unknown_ops}")

    est = PowerEstimate(
        model=model,
        hop_ms=hop,
        frame_rate_hz=frame_rate,
        total_ops_per_frame=total_ops,
        energy_per_frame_nj=energy_per_frame_nj,
        power_mw=power_mw,
        current_ma=current_ma,
        energy_per_hour_mj=energy_per_hour_mj,
    )
    return est, breakdown


# ---------------------------------------------------------------------
# rho_act computation (offline) from clip CSV
# ---------------------------------------------------------------------
def clip_results_path(clips_root: Path, tag: str, model: str) -> Path:
    m = normalize_model_name(model)
    return clips_root / tag / f"clip_results_{m}_{tag}.csv"


def compute_rho_act_from_clip_csv(csv_path: Path) -> Tuple[float, int]:
    """
    rho_act = fraction of clips with pred == 1

    Returns:
      (rho_act, n_clips)
    """
    df = pd.read_csv(csv_path)
    if "pred" not in df.columns:
        raise ValueError(f"CSV missing required column 'pred': {csv_path}")

    # Be robust to pred as strings/bools/ints
    pred = df["pred"]
    if pred.dtype == object:
        pred = pred.astype(str).str.strip().str.lower().map({"1": 1, "0": 0, "true": 1, "false": 0})
    pred = pred.fillna(0).astype(int)

    n = int(len(pred))
    if n == 0:
        raise ValueError(f"No rows in clip CSV: {csv_path}")

    rho = float((pred == 1).mean())
    return rho, n


def discover_tags(clips_root: Path) -> List[str]:
    if not clips_root.exists():
        return []
    tags = [p.name for p in clips_root.iterdir() if p.is_dir()]
    return sorted(tags)


# ---------------------------------------------------------------------
# Plotting helpers (optional)
# ---------------------------------------------------------------------
def _try_import_matplotlib():
    import matplotlib.pyplot as plt  # noqa: F401
    return plt


def write_bar_plot(df: pd.DataFrame, x: str, y: str, title: str, ylabel: str, out_path: Path) -> None:
    plt = _try_import_matplotlib()
    plt.figure(figsize=(max(7, 0.7 * len(df)), 5))
    bars = plt.bar(df[x].astype(str), df[y])
    plt.ylabel(ylabel)
    plt.title(title)
    plt.grid(True, axis="y")
    for bar in bars:
        val = bar.get_height()
        plt.text(bar.get_x() + bar.get_width() / 2.0, val, f"{val:.3f}", ha="center", va="bottom", fontsize=9)
    plt.xticks(rotation=30, ha="right")
    plt.tight_layout()
    plt.savefig(out_path, dpi=200)
    plt.close()


# ---------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------
def main() -> None:
    ap = argparse.ArgumentParser(description="Offline power estimator for VAD testbench")
    ap.add_argument("--profile_dir", type=str, default="outputs/op_profiles",
                    help="Directory containing op profile JSON files (from scripts/run_vad.py)")
    ap.add_argument("--clips_root", type=str, default="outputs/clips",
                    help="Root directory containing clip result CSVs (outputs/clips/<TAG>/...)")

    ap.add_argument("--models", nargs="*", default=[],
                    help="Standalone model names to estimate (e.g., energy zcr energy_zcr webrtc2 webrtc3)")

    ap.add_argument("--cascade", nargs="*", default=[],
                    help="Cascade specs as 'stage1:stage2' (e.g., energy:webrtc2). "
                         "Average P = P1 + rho_act * P2. rho_act is computed from stage1 clip CSVs.")

    ap.add_argument("--tags", nargs="*", default=[],
                    help="Experiment tags (subdirectories under outputs/clips). "
                         "Used to compute rho_act per tag for cascades.")
    ap.add_argument("--auto_tags", action="store_true",
                    help="Automatically include all tags found under --clips_root (recommended when running all datasets/profiles).")

    ap.add_argument("--vdd", type=float, default=1.0, help="Supply voltage in volts (for current estimate)")
    ap.add_argument("--costs_json", type=str, default=None,
                    help="Optional JSON file overriding per-op costs (nJ/op). Keys must match op names in profiles.")

    ap.add_argument("--out_base", type=str, default="outputs/power_estimates",
                    help="Base directory to save results")
    ap.add_argument("--no_plots", action="store_true", help="Disable plot generation")
    ap.add_argument("--verbose", action="store_true")

    args = ap.parse_args()

    profile_dir = Path(args.profile_dir)
    clips_root = Path(args.clips_root)

    # Costs
    costs = dict(DEFAULT_COSTS)
    if args.costs_json:
        with open(args.costs_json, "r", encoding="utf-8") as f:
            user_costs = json.load(f)
        if not isinstance(user_costs, dict):
            raise ValueError("--costs_json must contain a JSON object mapping op->nJ/op")
        costs.update({str(k): float(v) for k, v in user_costs.items()})

    # Normalize requested models
    standalone_models = [normalize_model_name(m) for m in (args.models or [])]
    cascade_specs: List[Tuple[str, str]] = []
    for spec in args.cascade or []:
        if ":" not in spec:
            raise ValueError(f"Invalid --cascade entry '{spec}'. Expected 'stage1:stage2'")
        s1, s2 = spec.split(":", 1)
        cascade_specs.append((normalize_model_name(s1), normalize_model_name(s2)))

    if not standalone_models and not cascade_specs:
        raise SystemExit("Nothing to do. Provide --models and/or --cascade.")

    # Tags for rho_act computation (only needed for cascade)
    tags: List[str] = [t for t in (args.tags or []) if t]
    if args.auto_tags:
        tags = sorted(set(tags) | set(discover_tags(clips_root)))

    # Output directory
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    out_dir = Path(args.out_base) / f"run_{timestamp}"
    out_dir.mkdir(parents=True, exist_ok=True)

    # Pre-compute standalone power for any model referenced anywhere.
    all_models_needed = set(standalone_models)
    for s1, s2 in cascade_specs:
        all_models_needed.add(s1)
        all_models_needed.add(s2)

    estimates: Dict[str, PowerEstimate] = {}
    breakdowns: Dict[str, Dict[str, Dict[str, float]]] = {}

    for m in sorted(all_models_needed):
        op_prof = load_op_profile(m, profile_dir=profile_dir)
        hop = hop_ms_for_model(m)  # hardcoded per requirement
        est, br = estimate_power(op_prof, costs, vdd=args.vdd, hop_ms=hop, verbose=args.verbose)
        estimates[m] = est
        breakdowns[m] = br

    # -----------------------------
    # Standalone table
    # -----------------------------
    standalone_rows: List[dict] = []
    for m in standalone_models:
        e = estimates[m]
        standalone_rows.append({
            "model": e.model,
            "hop_ms": e.hop_ms,
            "frame_rate_hz": e.frame_rate_hz,
            "total_ops_per_frame": e.total_ops_per_frame,
            "energy_per_frame_nj": e.energy_per_frame_nj,
            "power_mw": e.power_mw,
            "current_ma": e.current_ma,
            "energy_per_hour_mj": e.energy_per_hour_mj,
        })

    df_standalone = pd.DataFrame(standalone_rows)
    if len(df_standalone):
        df_standalone.to_csv(out_dir / "standalone_power.csv", index=False)
        with open(out_dir / "standalone_breakdown.json", "w", encoding="utf-8") as f:
            json.dump(breakdowns, f, indent=2)
        print(f"[ok] wrote: {out_dir / 'standalone_power.csv'}")
        print(f"[ok] wrote: {out_dir / 'standalone_breakdown.json'}")

    # -----------------------------
    # Cascade table (per tag)
    # -----------------------------
    cascade_rows: List[dict] = []
    if cascade_specs:
        if not tags:
            raise SystemExit("Cascade requested but no tags provided. Use --tags ... or --auto_tags.")

        for tag in tags:
            for s1, s2 in cascade_specs:
                csv_path = clip_results_path(clips_root, tag, s1)
                if not csv_path.exists():
                    # Skip quietly but record as missing
                    cascade_rows.append({
                        "tag": tag,
                        "cascade": f"{s1}+{s2}",
                        "stage1": s1,
                        "stage2": s2,
                        "rho_act": None,
                        "n_clips": 0,
                        "note": f"missing_csv:{csv_path}",
                        "power_mw": None,
                        "current_ma": None,
                        "energy_per_hour_mj": None,
                    })
                    continue

                rho, n_clips = compute_rho_act_from_clip_csv(csv_path)
                p1 = estimates[s1].power_mw
                p2 = estimates[s2].power_mw
                pc = p1 + rho * p2
                ic = pc / args.vdd
                ec_mj = pc * 3600.0

                cascade_rows.append({
                    "tag": tag,
                    "cascade": f"{s1}+{s2}",
                    "stage1": s1,
                    "stage2": s2,
                    "rho_act": rho,
                    "n_clips": n_clips,
                    "note": "",
                    "power_mw": pc,
                    "current_ma": ic,
                    "energy_per_hour_mj": ec_mj,
                    "stage1_power_mw": p1,
                    "stage2_power_mw": p2,
                })

        df_cascade = pd.DataFrame(cascade_rows)
        df_cascade.to_csv(out_dir / "cascade_power_by_tag.csv", index=False)
        print(f"[ok] wrote: {out_dir / 'cascade_power_by_tag.csv'}")

        # Summary across tags (weighted by n_clips)
        summary_rows: List[dict] = []
        ok = df_cascade[df_cascade["rho_act"].notna()].copy()
        if len(ok):
            for cascade_name, g in ok.groupby("cascade"):
                weights = g["n_clips"].astype(float)
                wsum = float(weights.sum())
                if wsum <= 0:
                    continue
                rho_w = float((g["rho_act"].astype(float) * weights).sum() / wsum)
                p_w = float((g["power_mw"].astype(float) * weights).sum() / wsum)
                summary_rows.append({
                    "cascade": cascade_name,
                    "rho_act_weighted": rho_w,
                    "power_mw_weighted": p_w,
                    "current_ma_weighted": p_w / args.vdd,
                    "energy_per_hour_mj_weighted": p_w * 3600.0,
                    "total_clips": int(wsum),
                    "tags_included": int(g["tag"].nunique()),
                })
        df_summary = pd.DataFrame(summary_rows)
        if len(df_summary):
            df_summary.to_csv(out_dir / "cascade_summary_weighted.csv", index=False)
            print(f"[ok] wrote: {out_dir / 'cascade_summary_weighted.csv'}")

    # -----------------------------
    # Optional plots
    # -----------------------------
    if not args.no_plots:
        try:
            if len(df_standalone):
                write_bar_plot(
                    df_standalone.sort_values("power_mw", ascending=True),
                    x="model",
                    y="power_mw",
                    title="Standalone Estimated Power (mW)",
                    ylabel="Power (mW)",
                    out_path=out_dir / "standalone_power.png",
                )
                write_bar_plot(
                    df_standalone.sort_values("total_ops_per_frame", ascending=True),
                    x="model",
                    y="total_ops_per_frame",
                    title="Standalone Total Ops per Frame",
                    ylabel="Ops / frame",
                    out_path=out_dir / "standalone_ops.png",
                )
            if cascade_specs:
                df_cascade_ok = pd.read_csv(out_dir / "cascade_power_by_tag.csv")
                df_cascade_ok = df_cascade_ok[df_cascade_ok["rho_act"].notna()].copy()
                if len(df_cascade_ok):
                    # Plot average power per tag for each cascade (separate figure per cascade)
                    for cascade_name, g in df_cascade_ok.groupby("cascade"):
                        g2 = g.sort_values("power_mw", ascending=True)
                        write_bar_plot(
                            g2,
                            x="tag",
                            y="power_mw",
                            title=f"Cascade Estimated Power by Tag: {cascade_name}",
                            ylabel="Power (mW)",
                            out_path=out_dir / f"cascade_{cascade_name}_by_tag.png",
                        )
        except Exception as e:
            print(f"[warn] plotting failed: {e}")

    # Human-readable console summary
    if len(df_standalone):
        print("\nStandalone models:")
        print(df_standalone[["model", "power_mw", "current_ma", "energy_per_hour_mj"]].to_string(index=False))

    if cascade_specs:
        df_c = pd.read_csv(out_dir / "cascade_power_by_tag.csv")
        ok = df_c[df_c["rho_act"].notna()].copy()
        print("\nCascades (per tag, showing first 20 valid rows):")
        if len(ok):
            print(ok[["tag", "cascade", "rho_act", "power_mw"]].head(20).to_string(index=False))
        else:
            print("  (no valid cascade rows; missing CSVs?)")

    print(f"\n[done] outputs in: {out_dir}")


if __name__ == "__main__":
    main()
